{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage import transform,feature,exposure\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readTrafficSigns(rootpath):\n",
    "    images = [] # images\n",
    "    labels = [] # corresponding labels\n",
    "    for c in range(0,43):\n",
    "        prefix = rootpath + '/' + format(c, '05d') + '/' # subdirectory for class\n",
    "        gtFile = open(prefix + 'GT-'+ format(c, '05d') + '.csv') # annotations file\n",
    "        gtReader = csv.reader(gtFile, delimiter=';') # csv parser for annotations file\n",
    "        gtReader.next() # skip header\n",
    "        for row in gtReader:\n",
    "            images.append(plt.imread(prefix + row[0])) # the 1th column is the filename\n",
    "            labels.append(row[7]) # the 8th column is the label\n",
    "        gtFile.close()\n",
    "    return images, labels\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "\n",
    "    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "    return gray\n",
    "\n",
    "\n",
    "def showimg_n_hog(grayimg,hogImage):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 10), sharex=True, sharey=True)\n",
    "    ax1.axis('off')\n",
    "    ax1.imshow(grayimg)\n",
    "    ax1.set_title('Input image')\n",
    "    ax1.set_adjustable('box-forced')\n",
    "\n",
    "    ax2.axis('off')\n",
    "    ax2.imshow(hogImage, cmap=plt.cm.gray)\n",
    "    ax2.set_title('Histogram of Oriented Gradients')\n",
    "    ax1.set_adjustable('box-forced')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def get_csv(path):\n",
    "    return [os.path.join(path,f) for f in os.listdir(path) if f.endswith('.csv')]\n",
    "\n",
    "def loadtestimages_from_path(testpath):\n",
    "    print(\"[INFO] reading all test images from directory\\n\")\n",
    "    filename = testpath+\"/new.csv\"\n",
    "    raw_data = open(filename, 'rt')\n",
    "    reader = csv.reader(raw_data, delimiter=';')\n",
    "    reader.next()\n",
    "    testfiles = list(reader)\n",
    "    timg = []\n",
    "    testimg = []\n",
    "    for row in testfiles:\n",
    "        fname = os.path.join(testpath,row[0])\n",
    "        timg.append(fname)\n",
    "        testimg.append(plt.imread(fname))\n",
    "    return timg,testimg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Code : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training images and labels are loaded in variables ==> X,y\n",
      "[INFO] Number of training Images 39209 \n",
      "Number of Labels 39209\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(\"Image_n_Labels/trainImages.npy\") &  os.path.isfile(\"Image_n_Labels/trainLabels.npy\") :\n",
    "    X = np.load(\"Image_n_Labels/trainImages.npy\")\n",
    "    y = np.load(\"Image_n_Labels/trainLabels.npy\")\n",
    "    print(\"[INFO] Training images and labels are loaded in variables ==> X,y\")\n",
    "    print(\"[INFO] Number of training Images {} \\nNumber of Labels {}\".format(len(X), len(y)))\n",
    "    \n",
    "else:\n",
    "    trainImages, trainLabels =readTrafficSigns(\"/home/rupali/Desktop/project/GTSRB/dataset/GTSRB/Final_Training/Images\")\n",
    "    np.save(\"Image_n_Labels/trainImages.npy\",trainImages)\n",
    "    np.save(\"Image_n_Labels/trainLabels.npy\",trainLabels)\n",
    "    print(\"[INFO] training images and labels are read from the dataset directory\")\n",
    "    print(\"[INFO] training images saved to Image_n_Labels/trainingImages.npy for further use\")\n",
    "    print(\"[INFO] training labels saved to Image_n_Labels/trainingLabels.npy for further use\")\n",
    "    X = np.load(\"Image_n_Labels/trainImages.npy\")\n",
    "    y = np.load(\"Image_n_Labels/trainLabels.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading from .npy\n",
      "\n",
      "[INFO] DONE!loaded from .npy\n",
      "\n",
      "[INFO] Training images and labels are loaded in variables ==> X,y\n",
      "[INFO] Number of training Images 26 \n",
      "Number of Labels 26\n"
     ]
    }
   ],
   "source": [
    "if (os.path.isfile(\"Image_n_Labels/testimagenames.npy\") &  os.path.isfile(\"Image_n_Labels/testimages.npy\")):\n",
    "    print(\"[INFO] loading from .npy\\n\")\n",
    "    timg = np.load(\"Image_n_Labels/testimagenames.npy\")\n",
    "    testimg = np.load(\"Image_n_Labels/testimages.npy\")\n",
    "    print(\"[INFO] DONE!loaded from .npy\\n\")\n",
    "    print(\"[INFO] Training images and labels are loaded in variables ==> X,y\")\n",
    "    print(\"[INFO] Number of training Images {} \\nNumber of Labels {}\".format(len(timg), len(testimg)))\n",
    "else:\n",
    "    testpath=\"/home/rupali/Desktop/project/GTSRB/dataset/GTSRB_test/Final_Test/small\"\n",
    "    timg,testimg = loadtestimages_from_path(testpath)\n",
    "    np.save(\"Image_n_Labels/testimagenames.npy\",timg)\n",
    "    np.save(\"Image_n_Labels/testimages.npy\",testimg)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract HoG features over all training images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading from file ... \n",
      "HoG features are loaded from HoGfeatures.npy to variable ==> hogfeat\n",
      "HoG visualizations are loaded from HoGvisualize.npy to variable ==> hogviz\n",
      "(39209, 2916)\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(\"HoGFeatures/HoGfeatures.npy\") & os.path.isfile(\"HoGFeatures/HoGvisualize.npy\") :\n",
    "    print(\"[INFO] loading from file ... \")\n",
    "    hogfeat = np.load(\"HoGFeatures/HoGfeatures.npy\")\n",
    "    hogviz = np.load(\"HoGFeatures/HoGvisualize.npy\")\n",
    "    \n",
    "    print(\"HoG features are loaded from HoGfeatures.npy to variable ==> hogfeat\")\n",
    "    print(\"HoG visualizations are loaded from HoGvisualize.npy to variable ==> hogviz\")\n",
    "    \n",
    "else:\n",
    "    Hviz=[]\n",
    "    Hfeat=[]\n",
    "    print(\"[INFO] HoGfeatures.npy does not exist\")\n",
    "    for i in range(len(X)):\n",
    "        if i > 0 and i % 1000 == 0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i, len(X)))\n",
    "        i1 = X[i]\n",
    "        grayim = rgb2gray(i1)\n",
    "        gI1 = transform.resize(grayim,(40,40))\n",
    "        (H, hogImage) = feature.hog(gI1, orientations=9, pixels_per_cell=(4,4),cells_per_block=(2, 2), transform_sqrt=True, visualise=True)\n",
    "        hogImage = exposure.rescale_intensity(hogImage, out_range=(0, 255)).astype(\"uint8\")\n",
    "\n",
    "        Hviz.append(hogImage)\n",
    "        Hfeat.append(H)\n",
    "\n",
    "    np.save(\"HoGFeatures/HoGfeatures.npy\", Hfeat)\n",
    "    np.save(\"HoGFeatures/HoGvisualize.npy\", Hviz)\n",
    "    print(\"[INFO] HoGfeatures.npy are saved\")  \n",
    "    print(\"[INFO] HoGvisualize.npy are saved\")\n",
    "    hogfeat = np.load(\"HoGFeatures/HoGfeatures.npy\")\n",
    "    hogviz = np.load(\"HoGFeatures/HoGvisualize.npy\")\n",
    "print (hogfeat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract HoG features over all Testing images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HoG features are loaded from HoGfeatures_test.npy to variable ==> hogfeat_test\n",
      "HoG visualizations are loaded from HoGvisualize_test.npy to variable ==> hogviz_test\n",
      "(26, 2916)\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(\"HoGFeatures/HoGfeatures_test.npy\") & os.path.isfile(\"HoGFeatures/HoGvisualize_test.npy\") :\n",
    "    hogfeat_test = np.load(\"HoGFeatures/HoGfeatures_test.npy\")\n",
    "    hogviz_test = np.load(\"HoGFeatures/HoGvisualize_test.npy\")\n",
    "    \n",
    "    print(\"HoG features are loaded from HoGfeatures_test.npy to variable ==> hogfeat_test\")\n",
    "    print(\"HoG visualizations are loaded from HoGvisualize_test.npy to variable ==> hogviz_test\")\n",
    "else:\n",
    "    print(\"HoGfeatures_test.npy does not found\")\n",
    "    Hviz = []\n",
    "    Hfeat = []\n",
    "    for i in range(0,len(testimg)):\n",
    "        # show an update every 1,000 images\n",
    "        if i > 0 and i % 10 == 0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i, len(testimg)))\n",
    "        I = testimg[i]\n",
    "        grayim = rgb2gray(I)\n",
    "        grayim = transform.resize(grayim,(40,40))\n",
    "\n",
    "        (H_4x4, hogImage) = feature.hog(grayim, orientations=9, pixels_per_cell=(4, 4),\n",
    "            cells_per_block=(2, 2), transform_sqrt=True, visualise=True)\n",
    "        hogImage = exposure.rescale_intensity(hogImage, out_range=(0, 255)).astype(\"uint8\")\n",
    "        Hviz.append(hogImage)\n",
    "        Hfeat.append(H_4x4)\n",
    "        # save the features using numpy save with .npy extention \n",
    "        # which reduced the storage space by 4times compared to pickle\n",
    "    np.save(\"HoGFeatures/HoGfeatures_test.npy\", Hfeat)\n",
    "    np.save(\"HoGFeatures/HoGvisualize_test.npy\", Hviz)\n",
    "    print(\"HoGfeatures_test.npy are saved\")  \n",
    "    print(\"HoGvisualize_test.npy are saved\")\n",
    "    hogfeat_test = np.load(\"HoGFeatures/HoGfeatures_test.npy\")\n",
    "    hogviz_test = np.load(\"HoGFeatures/HoGvisualize_test.npy\")\n",
    "print (hogfeat_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train and test dataset fromm training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data points: 26465\n",
      "validation data points: 2941\n",
      "testing data points: 9803\n"
     ]
    }
   ],
   "source": [
    "Xhog = np.array(hogfeat).astype(\"float\")\n",
    "y = y.astype(\"float\")\n",
    "X_t = np.array(hogfeat_test).astype(\"float\")\n",
    "\n",
    "\n",
    "features = Xhog\n",
    "labels = y\n",
    "Xtest = X_t\n",
    "\n",
    "# take the  data and construct the training and testing split, using 75% of the\n",
    "# data for training and 25% for testing\n",
    "(trainData, testData, trainLabels, testLabels) = train_test_split(features,\n",
    "    labels, test_size=0.25, random_state=42)\n",
    " \n",
    "# now, let's take 10% of the training data and use that for validation\n",
    "(trainData, valData, trainLabels, valLabels) = train_test_split(trainData, trainLabels,\n",
    "    test_size=0.1, random_state=84)\n",
    " \n",
    "# show the sizes of each data split\n",
    "print(\"training data points: {}\".format(len(trainLabels)))\n",
    "print(\"validation data points: {}\".format(len(valLabels)))\n",
    "print(\"testing data points: {}\".format(len(testLabels)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] pre-trained classifier not found. \n",
      " Training Classifier \\MLP = 200\n",
      "[INFO] Succefully trained the classsifier. \n",
      " Saving the classifier for further use\n",
      "[INFO] Classifier Saved\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(\"clf/clf_mlp_hog.pkl\"):\n",
    "    print(\"[INFO] loading classifier: MLP =200 trained on HoG features...\")\n",
    "    mlp= joblib.load(\"clf/clf_mlp_hog.pkl\")\n",
    "    print(\"[INFO] Classifer is loaded as instance ::mlp::\")\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"[INFO] pre-trained classifier not found. \\n Training Classifier \\MLP = 200\")\n",
    "#     Single hidden layer with 200 hidden neurons\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(200,))\n",
    "    mlp.fit(trainData,trainLabels)\n",
    "    print(\"[INFO] Succefully trained the classsifier. \\n Saving the classifier for further use\")\n",
    "    joblib.dump(mlp, 'clf/clf_mlp_hog.pkl') \n",
    "    print(\"[INFO] Classifier Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training data: 1.0\n",
      "accuracy on test data: 0.975925737019\n",
      "accuracy on validation data: 0.973478408705\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy on training data: {}\".format(mlp.score(trainData,trainLabels)))\n",
    "\n",
    "print(\"accuracy on test data: {}\".format(mlp.score(testData,testLabels)))\n",
    "\n",
    "print(\"accuracy on validation data: {}\".format(mlp.score(valData,valLabels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean cross-validation score: 0.95856783507\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'svc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ac0dea01604c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mean cross-validation score: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msvc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'svc' is not defined"
     ]
    }
   ],
   "source": [
    "cv_score = cross_val_score(mlp,testData,testLabels,cv=5)\n",
    "print(\"mean cross-validation score: {}\".format(np.mean(cv_score)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  4.,  1.,  2.,  4.,  3.,  2.,  2.,  4.,  1.,  4.,  2.,  2.,\n",
       "        1.,  4.,  2.,  2.,  2.,  2.,  2.,  4.,  4.,  1.,  1.,  1.,  2.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION ON TESTING DATA\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.98      0.93      0.95        44\n",
      "        1.0       0.94      0.96      0.95       594\n",
      "        2.0       0.92      0.93      0.93       560\n",
      "        3.0       0.94      0.95      0.95       348\n",
      "        4.0       0.98      0.98      0.98       529\n",
      "        5.0       0.91      0.88      0.89       462\n",
      "        6.0       1.00      0.99      0.99        81\n",
      "        7.0       0.94      0.94      0.94       336\n",
      "        8.0       0.95      0.96      0.95       385\n",
      "        9.0       1.00      0.99      1.00       359\n",
      "       10.0       0.99      0.99      0.99       461\n",
      "       11.0       0.98      1.00      0.99       291\n",
      "       12.0       0.99      1.00      1.00       548\n",
      "       13.0       1.00      0.99      1.00       549\n",
      "       14.0       1.00      0.99      1.00       197\n",
      "       15.0       0.99      1.00      1.00       152\n",
      "       16.0       1.00      1.00      1.00       108\n",
      "       17.0       1.00      1.00      1.00       286\n",
      "       18.0       0.99      0.99      0.99       288\n",
      "       19.0       1.00      1.00      1.00        53\n",
      "       20.0       1.00      0.92      0.96        90\n",
      "       21.0       0.98      1.00      0.99        82\n",
      "       22.0       0.99      1.00      1.00       107\n",
      "       23.0       1.00      0.99      0.99       138\n",
      "       24.0       1.00      0.99      0.99        68\n",
      "       25.0       0.98      0.99      0.99       369\n",
      "       26.0       0.99      0.98      0.98       152\n",
      "       27.0       0.98      1.00      0.99        60\n",
      "       28.0       0.99      0.98      0.99       129\n",
      "       29.0       0.99      0.96      0.97        77\n",
      "       30.0       0.97      0.99      0.98       118\n",
      "       31.0       1.00      1.00      1.00       201\n",
      "       32.0       1.00      1.00      1.00        48\n",
      "       33.0       1.00      0.99      0.99       182\n",
      "       34.0       1.00      0.97      0.99       117\n",
      "       35.0       0.99      0.99      0.99       290\n",
      "       36.0       1.00      0.98      0.99       105\n",
      "       37.0       1.00      0.98      0.99        60\n",
      "       38.0       0.99      1.00      1.00       517\n",
      "       39.0       0.99      1.00      0.99        69\n",
      "       40.0       0.97      0.99      0.98        77\n",
      "       41.0       1.00      1.00      1.00        60\n",
      "       42.0       1.00      1.00      1.00        56\n",
      "\n",
      "avg / total       0.98      0.98      0.98      9803\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = mlp.predict(testData)\n",
    " \n",
    "# show a final classification report demonstrating the accuracy of the classifier\n",
    "print(\"EVALUATION ON TESTING DATA\")\n",
    "print(classification_report(testLabels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
